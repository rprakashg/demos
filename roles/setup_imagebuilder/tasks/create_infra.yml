---      
- name: generate tf scripts
  block:
  - name: create temp directory to store terraform scripts
    ansible.builtin.tempfile:
      state: directory
      suffix: terraform
    register: tfdir
  
  - name: debug tfdir path
    ansible.builtin.debug:
      var: tfdir.path

  - name: create tf script
    ansible.builtin.copy:
      dest: "{{ tfdir.path }}/main.tf"
      content: | 
        provider "aws" {
          region = var.region

          # Make it faster by skipping something
          skip_metadata_api_check     = true
          skip_region_validation      = true
          skip_credentials_validation = true

          # skip_requesting_account_id should be disabled to generate valid ARN in apigatewayv2_api_execution_arn
          skip_requesting_account_id = false
        }
        
        provider "aws" {
          region = "us-east-1"
          
          alias = "acm"
    
          # Make it faster by skipping something
          skip_metadata_api_check     = true
          skip_region_validation      = true
          skip_credentials_validation = true

          # skip_requesting_account_id should be disabled to generate valid ARN in apigatewayv2_api_execution_arn
          skip_requesting_account_id = false
        }

        provider "random" {}

        resource "random_uuid" "unique_guid" {}

        data "aws_availability_zones" "available" {}
        data "aws_caller_identity" "current" {}
        data "aws_route53_zone" "this" {
          name = var.base_domain
        }
        data "aws_canonical_user_id" "current" {}
        data "aws_cloudfront_log_delivery_canonical_user_id" "cloudfront" {}

        data "aws_ami" "latest_rhel9_ami" {
          most_recent = true
          owners      = ["309956199498"] # Red Hat owner ID

          filter {
            name   = "name"
            values = ["RHEL-9.0.0_HVM-*"]
          }

          filter {
            name   = "architecture"
            values = ["x86_64"]
          }
        }

        resource "aws_key_pair" "sshkeypair" {
          key_name   = var.ssh_key
          public_key = file("~/.ssh/${var.ssh_key}.pub")
        }

        locals {
            azs                     = slice(data.aws_availability_zones.available.names, 0, 2)
            vpc_cidr                = "10.0.0.0/16" 
            account_id              = data.aws_caller_identity.current.account_id
            iso_bucket              = "${var.iso_bucket_name}-${random_uuid.unique_guid.result}"
            ami_bucket              = "${var.ami_bucket_name}-${random_uuid.unique_guid.result}"
            ostree_bucket           = "${var.ostree_bucket_name}-${random_uuid.unique_guid.result}"

            tags = {
                stack: var.stack
            }
        }

        # variables
        variable "region" {
          description = "AWS region"
          type        = string
        }

        variable "stack" {
          description = "Name of Stack"
          type        = string
        }

        variable instance_name {
          description = "EC2 instance name"
          type        = string
        }

        variable "instance_type" {
          description = "EC2 instance type to use"  
          type = string
        }

        variable ssh_key {
          description = "SSH key pair name"
          type        = string
        }
        
        variable "admin_user" {
          description = "Default password for ec2-user account"
          type = string
        }

        variable "admin_password" {
          description = "Default password for ec2-user account"
          type = string
        }

        variable "my_ip" {
          description = "ISP public IP"
          type = string
        }

        variable "iso_bucket_name" {
          description = "S3 Storage bucket for storing downloadable ISO installers"
          type        = string
        }

        variable "ami_bucket_name" {
          description = "S3 bucket to store edge AMI images"
          type = string
        }
        
        variable "ostree_bucket_name" {
          description = "S3 bucket to storing OSTree content"
          type = string
        }

        variable "base_domain" {
          description = "Base Route53 domain"
          type = string
        }

        variable "subdomain" {
          description = "OSTree subdomain"
          type = string
        }

        # Imagebuilder server
        module "ec2" {
          source  = "terraform-aws-modules/ec2-instance/aws"
          version = "2.8.0"

          name                          = var.instance_name
          instance_count                = 1
          
          instance_type                 = var.instance_type
          ami                           = data.aws_ami.latest_rhel9_ami.id
          subnet_id                     = tolist(module.vpc.public_subnets)[0]
          key_name                      = var.ssh_key
          vpc_security_group_ids        = [module.public_subnet_sg.security_group_id]
          associate_public_ip_address   = true
          ipv6_addresses = null
          private_ips = ["10.0.3.141"]

          iam_instance_profile = aws_iam_instance_profile.imagebuilder_instance_profile.name

          root_block_device = [
            {
              volume_type = "gp2"
              volume_size = 100,
            },
          ]

          ebs_block_device = [
            {
              device_name = "/dev/sdf"
              volume_type = "gp2"
              volume_size = 50
              encrypted   = false
            }
          ]

          tags = local.tags

          user_data = <<-EOF
            #!/bin/bash
            sudo yum -y update
            sudo dnf -y install rhel-system-roles ansible-core yum-utils

            sudo useradd -m ${var.admin_user}
            sudo usermod -aG wheel ${var.admin_user}
            sudo sed -i -e 's/^# %wheel/%wheel/' -e 's/^%wheel/# %wheel/' /etc/sudoers
            sudo sed -i -e 's/^%wheel/# %wheel/' -e 's/^# %wheel/%wheel/' /etc/sudoers
            sudo -u ${var.admin_user} mkdir -p /home/${var.admin_user}/.ssh
            sudo -u ${var.admin_user} bash -c "echo '${aws_key_pair.sshkeypair.public_key}' > /home/${var.admin_user}/.ssh/authorized_keys"
            sudo -u ${var.admin_user} ssh-keygen -t rsa -f /home/admin/.ssh/id_rsa -N ""
            sudo -u ${var.admin_user} cat .ssh/id_rsa.pub >> .ssh/authorized_keys
            sudo -u ${var.admin_user} chmod 700 /home/${var.admin_user}/.ssh
            sudo -u ${var.admin_user} chmod 600 /home/${var.admin_user}/.ssh/authorized_keys
            sudo usermod --password $(echo ${var.admin_password} | openssl passwd -1 -stdin) ${var.admin_user}
            sudo sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config
            sudo systemctl restart sshd
          EOF 
        }

        # Create an IAM Role for EC2 to access S3
        resource "aws_iam_role" "imagebuilder_access_role" {
          name = "imagebuilder_access_role"

          assume_role_policy = jsonencode({
            Version = "2012-10-17",
            Statement = [
              {
                Action = "sts:AssumeRole"
                Effect = "Allow"
                Principal = {
                  Service = "ec2.amazonaws.com"
                }
              }
            ]
          })
        }

        # imagebuilder host permissions to access AWS resources and services 
        resource "aws_iam_policy" "imagebuilder_access_policy" {
          name        = "imagebuilder_access_policy"
          description = "Imagebuilder instance profile role policy"

          policy = jsonencode({
            Version = "2012-10-17",
            Statement = [
              {
                Effect   = "Allow"
                Action   = "s3:*"
                Resource = [
                  "arn:aws:s3:::${local.iso_bucket}",
                  "arn:aws:s3:::${local.iso_bucket}/*",
                  "arn:aws:s3:::${local.ami_bucket}",
                  "arn:aws:s3:::${local.ami_bucket}/*",
                  "arn:aws:s3:::${local.ostree_bucket}",
                  "arn:aws:s3:::${local.ostree_bucket}/*",
                ]
              },
              {
                Effect  = "Allow",
                Action  = [
                  "ecr:GetAuthorizationToken",
                  "ecr:BatchCheckLayerAvailability",
                  "ecr:InitiateLayerUpload",
                  "ecr:UploadLayerPart",
                  "ecr:CompleteLayerUpload",
                  "ecr:PutImage",
                  "ecr:CreateRepository",
                  "ecr:DescribeRepositories",
                  "ecr:PutLifeCyclePolicy",
                  "ecr:TagResource",
                  "ecr:ListTagsForResource",
                  "ecr:ListImages",
                  "ecr:BatchGetImage",
                  "ecr:GetDownloadUrlForLayer"
                ],
                Resource: "*"
              },
              {
                Effect  = "Allow",
                Action  = [
                  "ec2:DescribeSnapshots",
                  "ec2:ImportSnapshot",
                  "ec2:DescribeImportSnapshotTasks",
                  "ec2:DescribeRegions",
                  "ec2:RegisterImage",
                  "ec2:CreateTags"
                ],
                Resource  = "*"
              }, 
              {
                Effect  = "Allow",
                Action  = [
                  "s3:ListAllMyBuckets"
                ],
                Resource = "*"
              },
              {
                Effect = "Allow",
                Action = [
                  "cloudfront:CreateInvalidation",
                  "cloudfront:GetDistribution",
                  "cloudfront:GetInvalidation",
                  "cloudfront:ListDistributions"
                ],
                "Resource": module.cloudfront.cloudfront_distribution_arn
              }
            ]
          })
        }

        # Attach the IAM policy to the IAM Role
        resource "aws_iam_role_policy_attachment" "imagebuilder_access_policy_attachment" {
          role       = aws_iam_role.imagebuilder_access_role.name
          policy_arn = aws_iam_policy.imagebuilder_access_policy.arn
        }

        # Create an IAM instance profile for the EC2 instance
        resource "aws_iam_instance_profile" "imagebuilder_instance_profile" {
          name = "imagebuilder_instance_profile"
          role = aws_iam_role.imagebuilder_access_role.name
        }

        # Create a vmimport service role
        resource "aws_iam_role" "vmimport_service_role" {
          name = "vmimport"

          assume_role_policy = jsonencode({
            Version = "2012-10-17",
            Statement = [
              {
                "Effect": "Allow",
                "Principal": {
                  "Service": "vmie.amazonaws.com"
                },
                "Action": "sts:AssumeRole",
                "Condition": {
                  "StringEquals": {
                    "sts:Externalid": "vmimport"  
                  }
                }
              }
            ]
          })
        }

        # Create vmimport service role policy
        resource "aws_iam_policy" "vmimport_service_role_policy" {
          name        = "vmimport_service_role_policy"
          description = "vmimport service role policy"

          policy = jsonencode({
            Version = "2012-10-17",
            Statement = [
              {
                Effect  = "Allow",
                Action  = [
                  "s3:GetBucketLocation",
                  "s3:GetObject",
                  "s3:PutObject"
                ],
                Resource: [
                  "arn:aws:s3:::${local.ami_bucket}",
                  "arn:aws:s3:::${local.ami_bucket}/*"
                ]
              },
              {
                Effect  = "Allow",
                Action  = [
                  "s3:GetBucketLocation",
                  "s3:GetObject",
                  "s3:ListBucket",
                  "s3:PutObject",
                  "s3:GetBucketAcl"
                ],
                Resource: [
                  "arn:aws:s3:::${local.ami_bucket}",
                  "arn:aws:s3:::${local.ami_bucket}/*"
                ]
              },
              {
                Effect  = "Allow",
                Action  = [
                  "ec2:ModifySnapshotAttribute",
                  "ec2:CopySnapshot",
                  "ec2:RegisterImage",
                  "ec2:Describe*"
                ],
                Resource  = "*"
              }
            ]
          })
        }

        # Attach the vmimport service role policy to vmimport service role
        resource "aws_iam_role_policy_attachment" "vmimport_service_role_policy_attachment" {
          role = aws_iam_role.vmimport_service_role.name
          policy_arn = aws_iam_policy.vmimport_service_role_policy.arn
        }

        # output variables
        output "imagebuilder_ip" {
          description = "Public IP address of the EC2 instance"
          value = module.ec2.public_ip[0]
        }

        output "imagebuilder_dns" {
          description = "Public DNS name of the EC2 instance"
          value = module.ec2.public_dns[0]
        }

        # Security Groups
        module "public_subnet_sg" {
          source  = "terraform-aws-modules/security-group/aws"
          version = "~> 5.0"

          name              = "${var.stack}-vpc-public-subnet-sg"
          description       = "Security group to allow HTTP/HTTPS, SSH access"
          vpc_id            = module.vpc.vpc_id

          # Ingress rules 1) allow SSH traffic from local machine 2) HTTP/HTTPS Traffic from any IP
          ingress_with_cidr_blocks = [
            {
              from_port = 22
              to_port   = 22
              protocol  = "tcp"
              description = "SSH Traffic from this machine"
              cidr_blocks = var.my_ip
            },
            {
              from_port = 80
              to_port   = 80
              protocol  = "tcp"
              description = "HTTP Traffic from any source"
              cidr_blocks = "0.0.0.0/0"
            },
            {
              from_port = 9090
              to_port   = 9090
              protocol  = "tcp"
              description = "Cockpit Traffic from any source"
              cidr_blocks = "0.0.0.0/0"
            },
            {
              from_port = -1
              to_port = -1
              protocol = "icmp"
              cidr_blocks = "0.0.0.0/0"
            },
            {
              from_port = 0
              to_port = 0
              protocol = "-1"
              cidr_blocks = "0.0.0.0/0"
            },
          ]

          #allow all outbound https traffic to internet
          egress_with_cidr_blocks = [
            {
              from_port = 0
              to_port   = 0
              protocol  = "-1"
              description = "All outbound traffic"
              cidr_blocks = "0.0.0.0/0"
            },
          ]
        }

        module "private_subnet_sg" {
          source  = "terraform-aws-modules/security-group/aws"
          version = "~> 5.0"

          name              = "${var.stack}-vpc-private-subnet-sg"
          description       = "Security group to allow HTTP/HTTPS, SSH access from only public subnet"
          vpc_id            = module.vpc.vpc_id
          
          # Ingress rules 1) allow SSH traffic from public subnet 2) HTTPS Traffic from public subnet
          ingress_with_source_security_group_id = [
            {
              from_port             = 22
              to_port               = 22
              protocol              = "tcp"
              description           = "SSH Traffic from public subnet"
              source_security_group_id = module.public_subnet_sg.security_group_id
            },
            {
              from_port             = 443
              to_port               = 443
              protocol              = "tcp"
              description           = "HTTPS Traffic from public subnet"
              source_security_group_id = module.public_subnet_sg.security_group_id
            },
            {
              from_port             = 80
              to_port               = 80
              protocol              = "tcp"
              description           = "HTTP Traffic from public subnet"
              source_security_group_id = module.public_subnet_sg.security_group_id
            },
          ]

          #allow all outbound https traffic to internet
          egress_with_cidr_blocks = [
            {
              from_port = 443
              to_port   = 443
              protocol  = "tcp"
              description = "HTTPS Traffic to any IP"
              cidr_blocks = "0.0.0.0/0"
            },
          ]
        }

        # Create an outbound rule on public subnet security group to allow ssh, http and https traffic flowing to private subnet
        resource "aws_security_group_rule" "allow_ssh_from_public_subnet" {
          type                      = "egress"
          security_group_id         = module.public_subnet_sg.security_group_id
          from_port                 = "22"
          to_port                   = "22"
          protocol                  = "tcp"
          cidr_blocks               = module.vpc.private_subnets_cidr_blocks
        }

        resource "aws_security_group_rule" "allow_http_from_public_subnet" {
          type                      = "egress"
          security_group_id         = module.public_subnet_sg.security_group_id
          from_port                 = "80"
          to_port                   = "80"
          protocol                  = "tcp"
          cidr_blocks               = module.vpc.private_subnets_cidr_blocks
        }

        resource "aws_security_group_rule" "allow_https_from_public_subnet" {
          type                      = "egress"
          security_group_id         = module.public_subnet_sg.security_group_id
          from_port                 = "443"
          to_port                   = "443"
          protocol                  = "tcp"
          cidr_blocks               = module.vpc.private_subnets_cidr_blocks
        }

        terraform {
          required_providers {
            aws = {
              source  = "hashicorp/aws"
              version = "5.63.0"
            }
          }
        }

        # VPC
        module "vpc" {
            source  = "terraform-aws-modules/vpc/aws"
            version = "5.8.1"

            name = "${var.stack}-vpc"

            cidr = local.vpc_cidr
            azs  = local.azs

            private_subnets         = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k)]
            public_subnets          = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 3)]
            
            create_database_subnet_group = false
            manage_default_network_acl    = false
            manage_default_route_table    = false
            manage_default_security_group = false

            enable_nat_gateway      = true
            single_nat_gateway      = true
            enable_dns_hostnames    = true

            # VPC Flow Logs (Cloudwatch log group and IAM role will be created)
            enable_flow_log                      = true
            create_flow_log_cloudwatch_log_group = true
            create_flow_log_cloudwatch_iam_role  = true
            flow_log_max_aggregation_interval    = 60

            tags = local.tags
        }

        # S3 bucket to store edge iso installers
        module "isos" {
          source  = "terraform-aws-modules/s3-bucket/aws"
          version = "4.1.2"

          bucket     = "${local.iso_bucket}"
          acl        = "public-read"

          # For example only
          force_destroy = true

          control_object_ownership  = true
          object_ownership          = "ObjectWriter"
          block_public_acls         = false
          attach_public_policy      = true
          block_public_policy       = false
          ignore_public_acls        = false
          restrict_public_buckets   = false
          
          versioning = {
            enabled = true
          }
          tags = local.tags
        }

        # S3 bucket to store edge ami images
        module "amis" {
          source  = "terraform-aws-modules/s3-bucket/aws"
          version = "4.1.2"

          bucket = "${local.ami_bucket}"
          acl    = "private"

          # For example only
          force_destroy = true

          control_object_ownership  = true
          object_ownership          = "ObjectWriter"

          versioning = {
            enabled = true
          }
          tags = local.tags
        }

        #S3 bucket to store ostree content
        module "ostree-repos" {
            source  = "terraform-aws-modules/s3-bucket/aws"
            version = "4.1.2"

            bucket  = "${local.ostree_bucket}"

            # For example only
            force_destroy = true

            tags = local.tags     
        }

        #log bucket for cdn
        module "log_bucket" {
          source  = "terraform-aws-modules/s3-bucket/aws"
          version = "4.1.2"

          bucket = "cdnlogs-${var.stack}"

          control_object_ownership = true
          object_ownership         = "ObjectWriter"

          grant = [{
            type       = "CanonicalUser"
            permission = "FULL_CONTROL"
            id         = data.aws_canonical_user_id.current.id
            }, {
            type       = "CanonicalUser"
            permission = "FULL_CONTROL"
            id         = data.aws_cloudfront_log_delivery_canonical_user_id.cloudfront.id
            # Ref. https://github.com/terraform-providers/terraform-provider-aws/issues/12512
            # Ref. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html
          }]
          force_destroy = true
        }

        #cert
        module "acm" {
          source  = "terraform-aws-modules/acm/aws"
          version = "~> 4.0"

          providers = {
            aws = aws.acm
          }  

          domain_name               = var.base_domain
          zone_id                   = data.aws_route53_zone.this.id
          subject_alternative_names = ["*.${var.base_domain}"]
        }

        # CDN for serving ostree content
        module "cloudfront" {
          source  = "terraform-aws-modules/cloudfront/aws"
          version = "3.4.1"
          
          enabled             = true
          aliases             = ["${var.subdomain}.${var.base_domain}"]
          staging             = false
          http_version        = "http2and3"     
          is_ipv6_enabled     = false
          price_class         = "PriceClass_All"
          retain_on_delete    = false
          wait_for_deployment = false

          continuous_deployment_policy_id = null

          create_origin_access_identity = true
          origin_access_identities = {
              s3_oai = "S3 Origin Access Identity for Cloudfront"
          }

          origin = {
              s3 = {
                  domain_name = module.ostree-repos.s3_bucket_bucket_regional_domain_name
                  s3_origin_config = {
                      origin_access_identity = "s3_oai" # key in `origin_access_identities`
                  }
              }
          }

          default_cache_behavior = {
              path_pattern           = "/*"
              target_origin_id       = "s3"
              viewer_protocol_policy = "allow-all"
              allowed_methods        = ["GET", "HEAD", "OPTIONS"]
              cached_methods         = ["GET", "HEAD"]
          }

          viewer_certificate = {
              acm_certificate_arn = module.acm.acm_certificate_arn
              ssl_support_method  = "sni-only"
          }

          logging_config = {
            bucket = module.log_bucket.s3_bucket_bucket_domain_name
            prefix = "cloudfront"
          }
        }

        # IAM Role and Policy definitions for CDN
        data "aws_iam_policy_document" "s3_policy" {
          # Origin Access Identities
          statement {
            actions   = ["s3:GetObject", "s3:ListBucket"]
            resources = [
                "${module.ostree-repos.s3_bucket_arn}", 
                "${module.ostree-repos.s3_bucket_arn}/*"
            ]
            principals {
              type        = "AWS"
              identifiers = module.cloudfront.cloudfront_origin_access_identity_iam_arns
            }
          }

          # Origin Access Controls
          statement {
            actions   = ["s3:GetObject", "s3:ListBucket"]
            resources = [
                "${module.ostree-repos.s3_bucket_arn}",
                "${module.ostree-repos.s3_bucket_arn}/*"
            ]
            principals {
              type        = "Service"
              identifiers = ["cloudfront.amazonaws.com"]
            }
            condition {
              test     = "StringEquals"
              variable = "aws:SourceArn"
              values   = [module.cloudfront.cloudfront_distribution_arn]
            }
          }
        }

        # Attach policy to bucket
        resource "aws_s3_bucket_policy" "bucket_policy" {
          bucket = module.ostree-repos.s3_bucket_id
          policy = data.aws_iam_policy_document.s3_policy.json
        }

        #R53 record for ostree subdomain
        module "records" {
          source  = "terraform-aws-modules/route53/aws//modules/records"
          version = "~> 2.0"

          zone_id = data.aws_route53_zone.this.zone_id

          records = [
            {
              name = var.subdomain
              type = "A"
              alias = {
                name    = module.cloudfront.cloudfront_distribution_domain_name
                zone_id = module.cloudfront.cloudfront_distribution_hosted_zone_id
              }
            },
          ]
        }

  - name: generate tfvars
    ansible.builtin.template:
      src: input.tfvars.j2
      dest: "{{ tfdir.path }}/input.tfvars"

- name: run tf 
  block:
  - name: init
    ansible.builtin.command:
      cmd: terraform init
      chdir: "{{ tfdir.path }}"
    register: tf_init_result
    ignore_errors: yes
      
  - name: plan
    ansible.builtin.command:
      cmd: terraform plan -var-file=input.tfvars
      chdir: "{{ tfdir.path }}"
    when: tf_init_result.rc == 0
    register: tf_plan_result
    ignore_errors: yes
  
  - name: apply
    ansible.builtin.command:
      cmd: terraform apply -auto-approve -var-file=input.tfvars
      chdir: "{{ tfdir.path }}"
    when: tf_plan_result.rc == 0
    register: tf_apply_result
    ignore_errors: yes
  
  - name: fail if errors provisioning
    ansible.builtin.fail:
      msg: "Failed to create infrastructure: \n {{ tf_apply_result.stderr }}"
    when: tf_apply_result.rc != 0
  
  - name: fetch public dns from output
    ansible.builtin.command:
      cmd: terraform output -raw imagebuilder_dns
      chdir: "{{ tfdir.path }}"
    register: tf_output_imagebuilder_dns
  
  - name: set fact
    set_fact:
      imagebuilder_dns: "{{ tf_output_imagebuilder_dns.stdout }}"

  - name: fetch public IP from output
    ansible.builtin.command:
      cmd: terraform output -raw imagebuilder_ip
      chdir: "{{ tfdir.path }}"
    register: tf_output_imagebuilder_ip
  
  - name: set fact
    set_fact:
      imagebuilder_ip: "{{ tf_output_imagebuilder_ip.stdout }}"

  - name: create inventory file
    ansible.builtin.copy:
      dest: ./inventory
      content: |
        ---
        all:
          hosts:
            {{ instance_name }}:
              ansible_host: "{{ imagebuilder_dns }}"
              ansible_port: 22
              ansible_user: "{{ admin_user }}"
              ansible_ssh_private_key_file: "{{ ansible_user_dir }}/.ssh/{{ ssh_key }}"

  - name: add imagebuilder host to inventory
    ansible.builtin.add_host:
      name: "{{ instance_name }}"
      hostname: "{{ imagebuilder_dns }}"
      groups:
      - "{{ instance_name }}"
      ansible_host: "{{ imagebuilder_dns }}"
      ansible_port: 22
      ansible_user: "{{ admin_user }}"
      ansible_ssh_private_key_file: "{{ ansible_user_dir }}/.ssh/{{ ssh_key }}"

  - name: wait for image builder host to be ready
    wait_for:
      host: "{{ imagebuilder_dns }}"
      port: 22
      delay: 10
      timeout: 300
      state: started